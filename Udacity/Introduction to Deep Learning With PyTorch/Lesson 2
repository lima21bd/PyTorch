# Higher Dimensions

> Number of features = Number of dimentions.

# Perceptrons

> A Perceptron is an algorithm used for supervised learning of binary classifiers. 
  Binary classifiers decide whether an input usually represented by a series of vectors belongs to a specific class.
  In short, a perceptron is a single-layer neural network. 
  They consist of four main parts including input values, weights and bias, net sum, and an activation function.

# Learning Rate

> Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.

# Perception Algorithm

> 1. Start with random weihts: w1, w2, ...., wn, b
  2. For every misclassified point (x1, x2, ...., xn):
        2.1. if prediction = 0:
                for i = 1, 2, ...., n:
                    change = wi + (alpha * xi)
                change b to (b + alpha)
                
        2.2. if prediction = 1:
                for i = 1, 2, ...., n:
                    change = wi - (alpha * xi)
                change b to (b - alpha)
                
   Here, negative class = 0
         positive positive = 1

# Error Function
> Error function tells us how far we are from the solution.

# log-loss function
> Which conditions should be met in order to apply gradient descent?
  1. The error function should be differentiable
  2. The error function should be continuous
 
# Discrete Prediction vs. Continuous Prediction
> Continuous Prediction is better than discrete prediction. For this, we need to switch from discrete to continuous predictions.

# Cross Entropy
> There's definitely a connection between probabilities and error functions. It's called Cross-Entropy.
* Bigger value of CE indicates bad model.
* Smaller value of CE indicates good model.

# Feedforward
> Feedforward is the process neural networks use to turn the input into an output.

# Backpropagation
> Backpropagation consists of:

  1. Doing a feedforward operation.
  2. Comparing the output of the model with the desired output.
  3. Calculating the error.
  4. Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.
  5. Use this to update the weights, and get a better model.
  6. Continue this until we have a model that is good.
  
# Overfitting & Underfitting
> Overfitting - High Variance, Large Coefficients
> Underfitting - High Bias

# Early Stopping
> We degrade in descent until the testing error stops decreasing and starts to increase. That moment we stop. This algorithm is called early stopping.

# Regularization
> Panalize large weights
> L1 regularization use Mode, Good for feature selection.
> L2 regularization use Square, Better for training models.
