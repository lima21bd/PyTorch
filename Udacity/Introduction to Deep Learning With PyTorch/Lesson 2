# Higher Dimensions

> Number of features = Number of dimentions.

# Perceptrons

> A Perceptron is an algorithm used for supervised learning of binary classifiers. 
  Binary classifiers decide whether an input usually represented by a series of vectors belongs to a specific class.
  In short, a perceptron is a single-layer neural network. 
  They consist of four main parts including input values, weights and bias, net sum, and an activation function.

# Learning Rate

> Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.

# Perception Algorithm

> 1. Start with random weihts: w1, w2, ...., wn, b
  2. For every misclassified point (x1, x2, ...., xn):
        2.1. if prediction = 0:
                for i = 1, 2, ...., n:
                    change = wi + (alpha * xi)
                change b to (b + alpha)
                
        2.2. if prediction = 1:
                for i = 1, 2, ...., n:
                    change = wi - (alpha * xi)
                change b to (b - alpha)
                
   Here, negative class = 0
         positive positive = 1

# Error Function

> Error function tells us how far we are from the solution.

# log-loss function

> Which conditions should be met in order to apply gradient descent?
  1. The error function should be differentiable
  2. The error function should be continuous
 
# Discrete Prediction vs. Continuous Prediction

> Continuous Prediction is better than discrete prediction. For this, we need to switch from discrete to continuous predictions.

# Cross Entropy
> Bigger value of CE indicates bad model.
> Smaller value of CE indicates good model.
